{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping com Python\n",
    "\n",
    "Tutorial de webscraping com pyhton utilizando a biblioteca BeautifulSoup\n",
    "\n",
    "### O que é webscraping?\n",
    "\n",
    "Webscraping é uma técnica de extração de dados, com ela podemos coletar dados de sites. Fazemos a 'raspagem' dos dados  que são interessantes para nós.\n",
    "\n",
    "Por exemplo: resgatar os últimos posts que foram escritos em vários blogs, assim sem precisarmos entrar nos sites alvo, podemos simplesmente iterar sobre eles e resgatar os dados, esta seria um exemplo simples da aplicação da técnica.\n",
    "\n",
    "Porém há muitas empresas que utilizam como forma de gerar recursos, um exemplo clássico é o site Buscapé, ele varre os sites que vendem os produtos pesquisados em busca dos menores preços.\n",
    "\n",
    "### Mais sobre o webscraping:\n",
    "\n",
    "Depois da extração dos dados, podemos armazenar eles de diversas formas:\n",
    "\n",
    "- Salvar em um banco de dados;\n",
    "- Salvar em CSV;\n",
    "- Salvar em XLS;\n",
    "- Entre outros;\n",
    "\n",
    "O processo em si é bem básico, definimos os dados que queremos, escolhemos os sites, montamos o script e recebemos os dados para análiser, este é o ciclo de vida do webscraping.\n",
    "\n",
    "Outro ponto legal é que se você souber um pouco de programação web vai se sentir muito confortável na exploração das tags, e na própria estrutura do html, isso já te deixa na frente.\n",
    "\n",
    "### Porque utilizar?\n",
    "\n",
    "Bom, vamos imaginar um caso: precisamos dos comentários de um produto X em 200 e-commerces diferentes para analisa-los e validar se o produto tem aceitação no mercado.\n",
    "\n",
    "Poderiamos entrar nos 200 sites copiar todos os comentários e salva-los numa planinha, por exemplo, mas e quantas horas gastariamos nisso? E se a pesquisa precisa ser repetida toda semana? Se faltou algum dado na primeira pesquisa?\n",
    "\n",
    "Teriamos que novamente nos submetermos a todo este processo manual, gastando horas para algo super simples!\n",
    "\n",
    "É aí que entra o webscraping, automatizamos todo o processo e só precisamos rodar o script até quando precisarmos, é muito mais vantajoso, e se feito de maneira correta se torna muito mais preciso também, a prova de erros humanos.\n",
    "\n",
    "### Antes de iniciarmos a prática\n",
    "\n",
    "Vou utilizar o Jupyter Notebook para demonstrar todos os exemplos, instalando o Anaconda você já consegue todas as libs usadas neste tutorial e inclusive o Notebook.\n",
    "\n",
    "Caso você opte por instalar as libs separadamente, faça isso com o pip, o resultado final será o mesmo, gosto da solução da Anaconda pois é prática e rápida, além de ser muito utilizada para tutoriais.\n",
    "\n",
    "Fiz um artigo de como instalar o Jupyter em diversas plataformas, caso queira aproveitar: [confira aqui!](https://medium.com/matheusbudkewicz/como-instalar-o-jupyter-notebook-windows-e-linux-20701fc583c)\n",
    "\n",
    "### Hello World em webscraping\n",
    "\n",
    "Vamos colocar a mão na massa para adicionar um pouco de prática, assim já conseguiremos abordar tópicos mais avançados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando as libs\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Definimos a url alvo. obs: esta url vem de um livro sobre o assunto do tópico\n",
    "url = 'http://pythonscraping.com/pages/page1.html'\n",
    "\n",
    "# Aplicamos uma requisição para pegar o HTML\n",
    "html = urlopen(url)\n",
    "\n",
    "# Verificação do conteúdo\n",
    "html.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronto!\n",
    "\n",
    "Temos o primeiro resultado por webscraping, foi fácil não?\n",
    "\n",
    "O retorno do método urlopen foi um HTML simples com diversos elementos que vemos todos os dias, o próximo passo seria extrair os dados que nós desejamos.\n",
    "\n",
    "Outro ponto importante é que no passo acima utilizamos a lib urllopen, nativa do Python, porém em outros tutoriais você pode se deparar com a Requests, que é bem conhecida e a comunidade abraça bem ela, então porque utilizamos a urllib?\n",
    "\n",
    "\n",
    "### Requests ou urllib?\n",
    "\n",
    "Bom, como vamos fazer requisições HTTP, no caso get, nas páginas que queremos extrair dados, precisamos de uma lib para isso, durante meus estudos sobre o tema estas duas apareceram de forma igual, então fui atrás para saber as vantagens e desvantagens de cada uma.\n",
    "\n",
    "Basicamente o que encontrei é que Requests é uma lib externa, no caso estariamos criando uma dependência para o projeto, outro ponto forte é a simplicidade de escrever o código, escrevemos menos linhas para chegar ao mesmo resultado comparando com a urllib, além disso há alguns comentários que dizem que Requests tem um código mais limpo e moderno que a outra.\n",
    "\n",
    "Já urllib é nativa do Python, o que quer dizer que provavelmente será mantida e pela mesma equipe que trabalha na linguagem, há alguns comentários pelo stackoverflow que falam sobre o desenvolvimento dela ser feito numa lógica de 'resolver o problema' ao invés de código limpo e performático.\n",
    "\n",
    "Neste post pretendo seguir com a urllib, mas teremos um exemplo com a Requests para você poder escolher a que mais te agrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando a Requests\n",
    "import requests\n",
    "\n",
    "# Vamos testar a biblioteca requests\n",
    "html = requests.get(url)\n",
    "\n",
    "# Diferente da urllib, usamos text para apresentar o conteudo que o get nos trouxe\n",
    "html.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que vimos como conseguir os dados de um site, está na hora de avançarmos ao próximo nível: usar uma biblioteca para poder manipular o HTML de uma forma mais fácil\n",
    "\n",
    "### BeautifulSoup\n",
    "\n",
    "Com a BeautifulSoup tudo será tranquilo, esta biblioteca do Python serve para extrairmos dados de HTML e XML, de forma fácil e descomplicada podemos acessar os 'nós' da estrutura do HTML da página e pegar as informações\n",
    "\n",
    "Vamos utilizar ela num exemplo para que fique claro o quanto mais fácil será se locomover pelas tags HTML ao invés de texto puro, como nos retornos dos métodos anteriores\n",
    "\n",
    "No próximo exemplo vou pegar o título da página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Welcome to Python.org</title>\n"
     ]
    }
   ],
   "source": [
    "# Importando a BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Vamos mudar a URL\n",
    "url = 'https://www.python.org/'\n",
    "\n",
    "# lendo a URL com a urllopen\n",
    "html = urlopen(url)\n",
    "\n",
    "# Enfim mostrando o poder da bs4\n",
    "bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Imprimindo o título da página\n",
    "print(bs.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validacao de erros\n",
    "\n",
    "http e server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
