{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping com Python\n",
    "\n",
    "Tutorial de webscraping com pyhton utilizando a biblioteca BeautifulSoup\n",
    "\n",
    "### O que é webscraping?\n",
    "\n",
    "Webscraping é uma técnica de extração de dados, com ela podemos coletar dados de sites. Fazemos a 'raspagem' dos dados  que são interessantes para nós.\n",
    "\n",
    "Por exemplo: resgatar os últimos posts que foram escritos em vários blogs, assim sem precisarmos entrar nos sites alvo, podemos simplesmente iterar sobre eles e resgatar os dados, esta seria um exemplo simples da aplicação da técnica.\n",
    "\n",
    "Porém há muitas empresas que utilizam como forma de gerar recursos, um exemplo clássico é o site Buscapé, ele varre os sites que vendem os produtos pesquisados em busca dos menores preços.\n",
    "\n",
    "### Mais sobre o webscraping:\n",
    "\n",
    "Depois da extração dos dados, podemos armazenar eles de diversas formas:\n",
    "\n",
    "- Salvar em um banco de dados;\n",
    "- Salvar em CSV;\n",
    "- Salvar em XLS;\n",
    "- Entre outros;\n",
    "\n",
    "O processo em si é bem básico, definimos os dados que queremos, escolhemos os sites, montamos o script e recebemos os dados para análiser, este é o ciclo de vida do webscraping.\n",
    "\n",
    "Outro ponto legal é que se você souber um pouco de programação web vai se sentir muito confortável na exploração das tags, e na própria estrutura do html, isso já te deixa na frente.\n",
    "\n",
    "### Porque utilizar?\n",
    "\n",
    "Bom, vamos imaginar um caso: precisamos dos comentários de um produto X em 200 e-commerces diferentes para analisa-los e validar se o produto tem aceitação no mercado.\n",
    "\n",
    "Poderiamos entrar nos 200 sites copiar todos os comentários e salva-los numa planinha, por exemplo, mas e quantas horas gastariamos nisso? E se a pesquisa precisa ser repetida toda semana? Se faltou algum dado na primeira pesquisa?\n",
    "\n",
    "Teriamos que novamente nos submetermos a todo este processo manual, gastando horas para algo super simples!\n",
    "\n",
    "É aí que entra o webscraping, automatizamos todo o processo e só precisamos rodar o script até quando precisarmos, é muito mais vantajoso, e se feito de maneira correta se torna muito mais preciso também, a prova de erros humanos.\n",
    "\n",
    "### Antes de iniciarmos a prática\n",
    "\n",
    "Vou utilizar o Jupyter Notebook para demonstrar todos os exemplos, instalando o Anaconda você já consegue todas as libs usadas neste tutorial e inclusive o Notebook.\n",
    "\n",
    "Caso você opte por instalar as libs separadamente, faça isso com o pip, o resultado final será o mesmo, gosto da solução da Anaconda pois é prática e rápida, além de ser muito utilizada para tutoriais.\n",
    "\n",
    "Fiz um artigo de como instalar o Jupyter em diversas plataformas, caso queira aproveitar: [confira aqui!](https://medium.com/matheusbudkewicz/como-instalar-o-jupyter-notebook-windows-e-linux-20701fc583c)\n",
    "\n",
    "### Hello World em webscraping\n",
    "\n",
    "Vamos colocar a mão na massa para adicionar um pouco de prática, assim já conseguiremos abordar tópicos mais avançados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando as libs\n",
    "from urllib.request import urlopen\n",
    "\n",
    "# Definimos a url alvo. obs: esta url vem de um livro sobre o assunto do tópico\n",
    "url = 'http://pythonscraping.com/pages/page1.html'\n",
    "\n",
    "# Aplicamos uma requisição para pegar o HTML\n",
    "html = urlopen(url)\n",
    "\n",
    "# Verificação do conteúdo\n",
    "html.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronto!\n",
    "\n",
    "Temos o primeiro resultado por webscraping, foi fácil não?\n",
    "\n",
    "O retorno do método urlopen foi um HTML simples com diversos elementos que vemos todos os dias, o próximo passo seria extrair os dados que nós desejamos.\n",
    "\n",
    "Outro ponto importante é que no passo acima utilizamos a lib urllopen, nativa do Python, porém em outros tutoriais você pode se deparar com a Requests, que é bem conhecida e a comunidade abraça bem ela, então porque utilizamos a urllib?\n",
    "\n",
    "\n",
    "### Requests ou urllib?\n",
    "\n",
    "Bom, como vamos fazer requisições HTTP, no caso get, nas páginas que queremos extrair dados, precisamos de uma lib para isso, durante meus estudos sobre o tema estas duas apareceram de forma igual, então fui atrás para saber as vantagens e desvantagens de cada uma.\n",
    "\n",
    "Basicamente o que encontrei é que Requests é uma lib externa, no caso estariamos criando uma dependência para o projeto, outro ponto forte é a simplicidade de escrever o código, escrevemos menos linhas para chegar ao mesmo resultado comparando com a urllib, além disso há alguns comentários que dizem que Requests tem um código mais limpo e moderno que a outra.\n",
    "\n",
    "Já urllib é nativa do Python, o que quer dizer que provavelmente será mantida e pela mesma equipe que trabalha na linguagem, há alguns comentários pelo stackoverflow que falam sobre o desenvolvimento dela ser feito numa lógica de 'resolver o problema' ao invés de código limpo e performático.\n",
    "\n",
    "Neste post pretendo seguir com a urllib, mas teremos um exemplo com a Requests para você poder escolher a que mais te agrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>A Useful Page</title>\\n</head>\\n<body>\\n<h1>An Interesting Title</h1>\\n<div>\\nLorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\\n</div>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando a Requests\n",
    "import requests\n",
    "\n",
    "# Vamos testar a biblioteca requests\n",
    "html = requests.get(url)\n",
    "\n",
    "# Diferente da urllib, usamos text para apresentar o conteudo que o get nos trouxe\n",
    "html.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora que vimos como conseguir os dados de um site, está na hora de avançarmos ao próximo nível: usar uma biblioteca para poder manipular o HTML de uma forma mais fácil\n",
    "\n",
    "### BeautifulSoup\n",
    "\n",
    "Com a BeautifulSoup tudo será tranquilo, esta biblioteca do Python serve para extrairmos dados de HTML e XML, de forma fácil e descomplicada podemos acessar os 'nós' da estrutura do HTML da página e pegar as informações\n",
    "\n",
    "Vamos utilizar ela num exemplo para que fique claro o quanto mais fácil será se locomover pelas tags HTML ao invés de texto puro, como nos retornos dos métodos anteriores\n",
    "\n",
    "No próximo exemplo vou pegar o título da página"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Welcome to Python.org</title>\n"
     ]
    }
   ],
   "source": [
    "# Importando a BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Vamos mudar a URL\n",
    "url = 'https://www.python.org/'\n",
    "\n",
    "# lendo a URL com a urllopen\n",
    "html = urlopen(url)\n",
    "\n",
    "# Enfim mostrando o poder da bs4\n",
    "bs = BeautifulSoup(html, 'lxml')\n",
    "\n",
    "# Imprimindo o título da página\n",
    "print(bs.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"site-headline\">\n",
      "<a href=\"/\"><img alt=\"python™\" class=\"python-logo\" src=\"/static/img/python-logo.png\"/></a>\n",
      "</h1>, <h1>Functions Defined</h1>, <h1>Compound Data Types</h1>, <h1>Intuitive Interpretation</h1>, <h1>Quick &amp; Easy to Learn</h1>, <h1>All the Flow You’d Expect</h1>]\n"
     ]
    }
   ],
   "source": [
    "# Imprimindo o título da página\n",
    "print(bs.find_all('h1'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introdução de HTML\n",
    "\n",
    "Vou apresentar uma rápida introdução ao HTML, caso você já se sinta confortável para entender a estrutura ou já trabalhou com desenvolvimento web, pule para a próxima parte. :)\n",
    "\n",
    "A seguir uma estrutura HTML simples:\n",
    "\n",
    "\n",
    "< html >\n",
    "< head >\n",
    "\t< title >Título do site< /title >\n",
    "< /head >\n",
    "< body >\n",
    "\t< h1 >Título da página< /h1 >\n",
    "\t< p >Um parágro com um conteúdo bem interessante!< /p >\n",
    "< /body >\n",
    "< /html >\n",
    "\n",
    "Toda página HTML tem pelo menos 3 tags que vão definir o esqueleto principal:\n",
    "\n",
    "- **< html >:** ela define que este é um documento HTML;\n",
    "- **< head >:** dentro destas tags serão colocadas configurações da página como encode, adicionados outros arquivos externos como scripts, definido o título da página ( o que aparece no browser ), entre outras que são como parâmetros para a página web;\n",
    "- **< body >:** esta tag contém todos os elementos visíveis da página, textos, listas, parágrafos e etc;\n",
    "\n",
    "Tudo que está entre sinais de < e > são tags, é por meio delas que estruturamos a página, a grande maioria precisa ser aberta e fechada, como no exemplo a seguir:\n",
    "\n",
    "< p >Texto do parágrafo< /p >\n",
    "\n",
    "Porém há algumas que não tem essa necessidade, mas não vem ao caso neste post, fogem do escopo.\n",
    "\n",
    "Como observado antes no nosso primeiro exemplo de webscraping, requisitamos title que é a tag title \n",
    "\n",
    "HTML é uma linguagem de marcação, não de programação, ela nos ajuda a estruturar as páginas, com elementos que podem representar blocos que dividem as páginas em partes (div), elementos de texto como título (h1), listas (ul), tabelas (table) e por aí vai, sempre seguindo esta lógica de cada tag conter um conteúdo diferente, lembrando que tudo que é visível fica dentro da tag body.\n",
    "\n",
    "As tags tambem aceitam parâmetros, os mais utilizados são classes e ids:\n",
    "\n",
    "- **classe:** o intuito de adicionar uma classe é que vários elementos possam aproveitar suas características\n",
    "- **id:** quando colocamos um id num elemento, ele tende a ser único na página, nas boas práticas dois elementos diferentes não podem ter o mesmo id\n",
    "\n",
    "Quando falo características me refiro a CSS ou manipulação do DOM com JS, não vou entrar nesse assunto também, mas é basicamente estilização dos elementos ou manipulação deles por meio destas duas linguagens.\n",
    "\n",
    "Voltando para o webscraping, as classes e ids nos auxiliarão a acessar os elementos que queremos resgatar os dados.\n",
    "\n",
    "Por exemplo: identificamos que um dado importante está em um parágrafo, se procurarmos por parágrafos podemos achar inúmeros elementos com essa tag, agora se o elemento alvo conter um id ele será apenas um, e isso facilitará muito nossa vida.\n",
    "\n",
    "Creio que com isso, o básico do HTML está exemplificado e além disso conseguimos estabelecer ligações entre o webscraping e a estrutura HTML, como iremos navegar para resgatar os dados!\n",
    "\n",
    "Vamos retornar para a BeautifulSoup\n",
    "\n",
    "### Métodos find() e find_all()\n",
    "\n",
    "O método find_all procura por todo o documento por resultados da nossa pesquisa, e nos retornas toda as ocorrências encontradas.\n",
    "\n",
    "Porém as vezes queremos encontrar apenas um elemento, o que nos dá duas possibilidades\n",
    "\n",
    "- passar o argumento limit = 1, no find_all()\n",
    "- utilizar o find()\n",
    "\n",
    "Com o find() o primeiro elemento encontrado é retornado, assim caso buscassemos uma tabela e soubermos que apenas uma tabela existe em todo o HTML é desnecessário utilizar o find_all() que leria o documento todo, assim por questões até de performance optariamos pelo find()\n",
    "\n",
    "Veremos agora os dois na prática:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-5b79881fd365>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-4-5b79881fd365>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    validacao de erros\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#find e find_all\n",
    "\n",
    "#find by class and id\n",
    "\n",
    "#css selectors\n",
    "\n",
    "validacao de erros\n",
    "\n",
    "http e server"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
